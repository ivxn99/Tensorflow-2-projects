{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corona BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbWwy4e4Uqxr"
      },
      "source": [
        "Dataset used: [Coronavirus tweets NLP](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21eziCizgQLp",
        "outputId": "62725396-071f-4eb4-974e-5e0e3a23db12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/c1/015648a2186b25c6de79d15bec40d3d946fcf1dd5067d1c1b28009506486/bert-for-tf2-0.14.6.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.7MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.6-cp36-none-any.whl size=30318 sha256=f1b00aded05779f58b8bb6f11a915723a95115748c2ca33da6d8bf3099cb52a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/a0/b4/75b0601ebaa41e517a797fe9cea119c789664c8408f8a74ae9\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=272a53cfade6c7aca0e0c2c4bfbfe41095d3a2233b8da36d8dadba2d05036d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=3cebf12476430c0d818e271c89dff3ac5b5da0041c767b5d6f8c2d022bf8e4c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.6 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPi1LQuffkzj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INuwPMIEfsuo"
      },
      "source": [
        "train_dir = '/content/Corona_NLP_train.csv'\n",
        "#test_dir = 'C:/Users/Dejan/Downloads/Corona_NLP_test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_dir, encoding='latin-1')\n",
        "#test_data = pd.read_csv(test_dir, encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnj3Ki94fssn",
        "outputId": "b03b57ec-d634-463d-92da-bb41d2237f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    #text = text.replace('\\%','')\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
        "    text = re.sub('([#])|([^a-zA-Z])', ' ', text)\n",
        "    #text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
        "    return text\n",
        "\n",
        "train_data['OriginalTweet'] = train_data['OriginalTweet'].apply(lambda x: clean_text(x))\n",
        "test_data['OriginalTweet'] = test_data['OriginalTweet'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4d92f245a76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OriginalTweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OriginalTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OriginalTweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OriginalTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfhZ2KHWfsog"
      },
      "source": [
        "print(\"Max tweet length:\", train_data['OriginalTweet'].map(len).max())\n",
        "print(\"Min tweet length:\", train_data['OriginalTweet'].map(len).min())\n",
        "print(\"Average tweet length:\", train_data['OriginalTweet'].map(len).mean())\n",
        "\n",
        "chars = sorted(list(set(train_data['OriginalTweet'])))\n",
        "print('Total characters:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5L9lUvfgE2I",
        "outputId": "840c9fed-a15f-4fae-8d80-72c66310f1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
        "MAX_SEQ_LEN = 500  # max sequence length\n",
        "\n",
        "\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1] * len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == '[SEP]':\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens, )\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[:max_len]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "    ids = get_ids(stokens, tokenizer)\n",
        "    masks = get_masks(stokens)\n",
        "    segments = get_segments(stokens)\n",
        "    return ids, masks, segments\n",
        "\n",
        "\n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "\n",
        "    for sentence in tqdm(sentences, position=0, leave=True):\n",
        "        ids, masks, segments = create_single_input(sentence, tokenizer, MAX_SEQ_LEN - 2)\n",
        "        assert len(ids) == MAX_SEQ_LEN\n",
        "        assert len(masks) == MAX_SEQ_LEN\n",
        "        assert len(segments) == MAX_SEQ_LEN\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32),\n",
        "            np.asarray(input_masks, dtype=np.int32),\n",
        "            np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "\n",
        "def create_tonkenizer(bert_layer):\n",
        "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "    return tokenizer\n",
        "\n",
        "def nlp_model(callable_object):\n",
        "    # Load the pre-trained BERT base model\n",
        "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)\n",
        "\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN, ), dtype=tf.int32, name=\"input_ids\")\n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN, ), dtype=tf.int32, name=\"input_masks\")\n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN, ), dtype=tf.int32, name='segment_ids')\n",
        "\n",
        "    inputs = [input_ids, input_masks, input_segments] #Bert inputs\n",
        "    pooled_output, sequence_output = bert_layer(inputs) #Bert outputs\n",
        "\n",
        "    x = Dense(768, activation='relu')(pooled_output)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    outputs = Dense(5, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            3845        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,076,678\n",
            "Trainable params: 110,076,677\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU-PH1JygEwI",
        "outputId": "9d974bc3-d41f-4441-a154-e119c22d2c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create examples for training and testing\n",
        "train_data = train_data.sample(frac=1) # Shuffle the dataset\n",
        "tokenizer = create_tonkenizer(model.layers[3])\n",
        "X_train = convert_sentences_to_features(train_data['OriginalTweet'][:37000], tokenizer)\n",
        "X_test = convert_sentences_to_features(train_data['OriginalTweet'][37000:], tokenizer)\n",
        "\n",
        "train_data['Sentiment'].replace('Extremely Negative',0.,inplace=True)\n",
        "train_data['Sentiment'].replace('Negative',1.,inplace=True)\n",
        "train_data['Sentiment'].replace('Neutral',2.,inplace=True)\n",
        "train_data['Sentiment'].replace('Positive',3.,inplace=True)\n",
        "train_data['Sentiment'].replace('Extremely Positive',4.,inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 37000/37000 [00:18<00:00, 2043.59it/s]\n",
            "100%|██████████| 4157/4157 [00:01<00:00, 2102.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik4ObUVkgLqY",
        "outputId": "0b990af7-76db-4cb4-a5d1-4c70b6abdaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "one_hot_encoded = to_categorical(train_data['Sentiment'].values)\n",
        "y_train = one_hot_encoded[:37000]\n",
        "y_test =  one_hot_encoded[37000:]\n",
        "\n",
        "batch_size = 4\n",
        "opt = Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data = (X_test, y_test),\n",
        "                    epochs=1,\n",
        "                    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9250/9250 [==============================] - 4746s 513ms/step - loss: 0.6877 - accuracy: 0.7321 - val_loss: 0.4769 - val_accuracy: 0.8239\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}